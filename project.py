# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ufn-vkpm--qXivZG--fAtK-KXjnhEIET
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd


df = pd.read_csv("PROJECT.csv")

df.head()

# Shape of the dataset
print("Dataset Shape:", df.shape)

#  missing values
print("Missing Values:\n")
print(df.isnull().sum())

# Dataset information
df.info()

df.describe()

from sklearn.preprocessing import LabelEncoder

# Create separate encoders for each column to avoid confusion
gender_encoder = LabelEncoder()
smoke_encoder = LabelEncoder()

# Encode 'gender'
df['gender'] = gender_encoder.fit_transform(df['gender'])

# Encode 'smoking_history'
df['smoking_history'] = smoke_encoder.fit_transform(df['smoking_history'])

# Optional: Save the mappings for interpretation
print("Gender Mapping:", dict(zip(gender_encoder.classes_, gender_encoder.transform(gender_encoder.classes_))))
print("Smoking History Mapping:", dict(zip(smoke_encoder.classes_, smoke_encoder.transform(smoke_encoder.classes_))))

# Check first few rows
df.head()

from sklearn.model_selection import train_test_split

# Select features and target
features = ['gender', 'age', 'hypertension', 'heart_disease', 'smoking_history',
            'bmi', 'HbA1c_level', 'blood_glucose_level']
X = df[features]
y = df['diabetes']  # Target variable

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression

#train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

# Predict on test set
y_pred = model.predict(X_test)

# Print accuracy and classification report
print("Model Accuracy:", accuracy_score(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Make predictions
rf_pred = rf_model.predict(X_test)

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict using Random Forest
rf_pred = rf_model.predict(X_test)

# Evaluate
from sklearn.metrics import accuracy_score, classification_report

print(" Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
print("\n Random Forest Classification Report:\n", classification_report(y_test, rf_pred))

# Calculate classification metrics
acc = accuracy_score(y_test, rf_pred)
prec = precision_score(y_test, rf_pred)
rec = recall_score(y_test, rf_pred)
f1 = f1_score(y_test, rf_pred)


print("Accuracy       :", acc)
print(" Precision     :", prec)
print(" Recall        :", rec)
print(" F1 Score      :", f1)

# Extract feature importances from the trained XGBoost model
importances = xgb_model.feature_importances_
feature_names = X_train.columns

# Create a DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Show the top features
print(importance_df)

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Train XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Predict
xgb_pred = xgb_model.predict(X_test)

# Evaluate
xgb_acc = accuracy_score(y_test, xgb_pred)
xgb_prec = precision_score(y_test, xgb_pred)
xgb_rec = recall_score(y_test, xgb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)

# Print results
print("XGBoost Accuracy  :", round(xgb_acc, 4))
print("XGBoost Precision :", round(xgb_prec, 4))
print("XGBoost Recall    :", round(xgb_rec, 4))
print("XGBoost F1 Score  :", round(xgb_f1, 4))


print("\nClassification Report:\n")
print(classification_report(y_test, xgb_pred))

sns.countplot(x=y)
plt.title("Distribution of Diabetes Cases")
plt.xlabel("Diabetes (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

sns.boxplot(x='diabetes', y='blood_glucose_level', data=df)
plt.title("Blood Glucose Level vs. Diabetes")
plt.xlabel("Diabetes")
plt.ylabel("Blood Glucose Level")
plt.show()

sns.histplot(data=df, x='age', hue='diabetes', bins=30, kde=True)
plt.title("Age Distribution by Diabetes Status")
plt.show()

sns.violinplot(x='diabetes', y='HbA1c_level', data=df)
plt.title("HbA1c Level vs Diabetes")
plt.show()

top_features = ['HbA1c_level', 'blood_glucose_level', 'age', 'heart_disease', 'diabetes']
sns.pairplot(df[top_features], hue='diabetes')
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score

# Predict using XGBoost model
xgb_pred = xgb_model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, xgb_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Diabetes", "Diabetes"])
disp.plot(cmap='Oranges')
plt.title(f"XGBoost Confusion Matrix\nAccuracy: {accuracy_score(y_test, xgb_pred):.4f}")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')
plt.title('Feature Importance from XGBoost Model')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

from sklearn.metrics import precision_recall_curve
probs = xgb_model.predict_proba(X_test)[:,1]
precision, recall, _ = precision_recall_curve(y_test, probs)
plt.plot(recall, precision)
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, auc
fpr, tpr, _ = roc_curve(y_test, probs)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.show()

sns.boxplot(x='diabetes', y='bmi', data=df)
plt.title("BMI Distribution by Diabetes")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

ax = sns.countplot(x='gender', hue='diabetes', data=df)
plt.title("Gender vs Diabetes Distribution")
plt.xlabel("Gender (0 = Male, 1 = Female, 2 = Other)")
plt.ylabel("Count")
plt.legend(title='Diabetes (0 = No, 1 = Yes)')

# Rename tick labels manually
ax.set_xticklabels(['Male', 'Female', 'Other'])

plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))  # Wider figure to make room
ax = sns.countplot(x='smoking_history', hue='diabetes', data=df)
plt.title("Smoking History vs Diabetes")
plt.xlabel("Smoking History (0â€“5)")
plt.ylabel("Count")
plt.xticks(rotation=45)

# Custom category names
ax.set_xticklabels([
    'Never', 'Former', 'Current', 'Ever', 'Not Available', 'Unknown'
])

# Move legend outside plot (right side)
plt.legend(
    title='Diabetes (0 = No, 1 = Yes)',
    bbox_to_anchor=(1.05, 1),  # X = 105%, Y = top
    loc='upper left'
)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt


metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
random_forest = [0.9704, 0.9507, 0.6893, 0.7992]
xgboost = [0.9716, 0.9605, 0.6963, 0.8073]

x = range(len(metrics))

plt.figure(figsize=(8, 6))
plt.bar(x, random_forest, width=0.4, label='Random Forest', align='center')
plt.bar([i + 0.4 for i in x], xgboost, width=0.4, label='XGBoost', align='center')

plt.xticks([i + 0.2 for i in x], metrics)
plt.ylim(0.6, 1.0)
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


total_samples = 100000
batch_size = 50
num_batches = total_samples // batch_size



import numpy as np
np.random.seed(42)
predicted_diabetics = np.random.randint(0, 10, size=num_batches)
batch_accuracies = np.round(np.random.uniform(0.9, 1.0, size=num_batches), 2)


simulation_full_df = pd.DataFrame({
    'Batch': range(1, num_batches + 1),
    'Total Samples': batch_size,
    'Predicted Diabetics': predicted_diabetics,
    'Batch Accuracy': batch_accuracies
})

simulation_full_df.head()

import gradio as gr
import pandas as pd
import numpy as np
import joblib
import datetime

# Simulated model and scaler loading (use your real .pkl files in actual deployment)
# xgb_model = joblib.load("xgb_model.pkl")
# scaler = joblib.load("scaler.pkl")

# For demo purposes: mock prediction function
def mock_predict(input_data):
    """Mock XGBoost model predict function"""
    # Simulate diabetic prediction: randomly decide
    prediction = np.random.choice([0, 1])
    probability = np.round(np.random.uniform(0.75, 0.99), 2)
    return prediction, probability

# Initialize twin record log
twin_log = []

# Inference function for Gradio
def predict_diabetes(age, gender, bmi, bp, hba1c):
    gender_encoded = 1 if gender == "Male" else 0

    # Input as DataFrame row
    input_data = pd.DataFrame([[age, gender_encoded, bmi, bp, hba1c]],
                              columns=['Age', 'Gender', 'BMI', 'BloodPressure', 'HbA1c'])

    # Use real model: scaled_input = scaler.transform(input_data)
    # prediction = xgb_model.predict(scaled_input)
    # probability = xgb_model.predict_proba(scaled_input)[0][1]

    prediction, probability = mock_predict(input_data)

    result = "Diabetic" if prediction == 1 else "Non-Diabetic"

    # Log entry
    entry = {
        "Time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "Age": age,
        "Gender": gender,
        "BMI": bmi,
        "BloodPressure": bp,
        "HbA1c": hba1c,
        "Prediction": result,
        "Confidence": probability
    }

    twin_log.append(entry)
    log_df = pd.DataFrame(twin_log[-5:])  # Show latest 5

    return f"Prediction: {result} (Confidence: {probability})", log_df

# Gradio Interface
demo = gr.Interface(
    fn=predict_diabetes,
    inputs=[
        gr.Slider(1, 100, value=45, label="Age"),
        gr.Radio(["Male", "Female"], label="Gender"),
        gr.Slider(10.0, 50.0, value=25.0, step=0.1, label="BMI"),
        gr.Slider(60, 180, value=90, label="Blood Pressure"),
        gr.Slider(4.0, 15.0, value=6.5, step=0.1, label="HbA1c (%)")
    ],
    outputs=[
        gr.Text(label="Digital Twin Prediction"),
        gr.Dataframe(label="Latest Twin Log")
    ],
    title="ðŸ§¬ Diabetes Digital Twin Monitor",
    description="Enter patient data to simulate real-time prediction and twin update."
)

demo.launch(share=True)

